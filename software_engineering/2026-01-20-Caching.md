# qubit-note: Distributed Systems Series | Caching Part 1 | Caching Methods

## Overview

We have discussed <a href="2025-08-04-partitioning.md">data partitioning</a> and <a href="2025-04-22-data-replication.md">data replication</a>
as a means of enhancing system availability and optimizing towards low latency. In this note we will discuss yet another technique namely caching.
Specifically we will birefly discuss the following appraoches to caching:

- Cache-aside caching
- Read-through caching
- Write-through caching
- Write-behind caching
- Client-side caching
- Distributed caching


**keywords** software-architecture, system-design, distributed-systems, caching, data-replication

## Caching Methods

Before we begin let's see what caching is. This is a technique whereby we maintain a temporary copy
of the data closer to where actually the data is accessed. Thus, apart from the main storage, when using caching,
we maintian one or more copies of the data to one or more locations [1]. With web applications,
when we cache data we don't copy the whole data but rather a small portion of it. Caching unavoidably brings up the
issue of data consistency i.e. how up to date the cached data is. We will discuss this  in part 2.

Now that we have an udenrstanding of what caching is let's see some appraoches to it.
For more informatio on these appraoches please see [1]

#### Cache-aside caching

This is probably the caching strategy most often used [1]. In this approach we maintain a cache
either as a separate serever, e.g. Redis, or even in the application space. When the item we are looking
for is in the cache, i.e. a cache hit, access latency is very small. When the item is not found the cache
reports a miss and the application queries the database. The application is also responsible for updating
the cache in this scenario.

#### Read-through caching

This is similar to cache-aside. The difference is that in read-through caching the cache is responsible for querying
the database and update itself. Thus, the cache in this scenario is not a passive component.

####  Write-through caching & write-behind caching

The two appraoches we discussed above are strategies for caching reads [1]. Often we want the cache to support writes.
When using cache-aside, the application is responsible for managing the writes as the cache is simply a passive component.
When, however, read-through caching is employed we have two options [1]:

- Write-through caching
- Write-behind caching

Write-through caching is a strategywhere an update to the cache propagates immediately, i.e. synchronously, to the backing store [1].
Thus, The write latency in this case is dominated by the write latency to the backing store, which can be significant [1].


With write-through caching we need to wait for the database store for an unknowledgement that the data has been written before updating the
cache. In contrast, write-behind updates the cache immediately. Hence, with write-behind caching, the cache may accept multiple updates before updating the database [1].


#### Client-side caching

Cache servers like Redis use in-memory caching. However, the application must communicate
with the server over the network via the Redis protocol. Depending on the application, this approach may not be appropriate
as the last-mile latency can still be significant [1]. 

Client-side caching places the cache layer within the application itself [1].
With client-side caching, a combination of read-through and write-behind caching is used so both reads and writes are fast [1].
Note however that client-side caching makes transactions hard to guarantee because of the database access indirection layers and latency [1].


#### Distributed caching

Finally, let's discuss distributed caching. Often there is a need for the cache to be distibuted. In this case, we use multiple cache nodes to reduce geographic latency and handle higher workloads. In this setup, the cached data is typically partitioned across nodes to avoid storing everything everywhere, and replicated to improve availability and reduce access latency. As a result, distributed caching combines the advantages, and also the complexities, of caching, partitioning, and replication, requiring careful design to manage these trade-offs effectively.

## Summary



This qubit note introduces caching as a core technique in distributed systems for improving performance and reducing latency by keeping temporary copies of frequently accessed data closer to where it is used. Unlike replication or partitioning, caching typically stores only a subset of data and raises important consistency concerns.

The note outlines several common caching strategies:

- Cache-aside caching: The application manages the cache, reading from it first and updating it on misses.
- Read-through caching: The cache actively fetches missing data from the database.
- Write-through caching: Writes update the cache and the database synchronously, ensuring consistency but increasing latency.
- Write-behind caching: Writes update the cache immediately and propagate to the database asynchronously, improving performance at the cost of temporary inconsistency.
- Client-side caching: The cache lives inside the application, minimizing network latency but complicating transactions and consistency.
- Distributed caching: Cache data is partitioned and replicated across multiple nodes to improve scalability, availability, and geographic latency, while inheriting the complexities of both partitioning and replication.

Overall, the note frames caching as a powerful but nuanced optimization technique that must balance performance gains against consistency, complexity, and system design trade-offs.

## References

1. Pekka Enberg, _Latency Reduce delay in software systems_ Manning Publications
2. Roberto Vitillo _Understanding Distributed Stystems. What every developer should know about large distributed apllications_
