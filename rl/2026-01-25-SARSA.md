# qubit-note: SARSA or State-Action-Reward-State-Action

## Overiew

<a href="2026-01-24-Temporal-Difference-Learning.md">qubit-note: Temporal Difference Learning</a> discussed temporal difference learning.
This is a compination of dynamic programming and Monte Carlo. TDL does not use the environment transition probabilities and therefore is
a model-free algorithm. One problem with TDL is that we don't which action to take; TDL approximate that state-value function $V(s)$
however, we cannot use the Bellman equations to establish the state-action value function $Q(s, \alpha)$ as we don't know the transition probabilities.
The solution thus is to approaximate $Q(s, \alpha)$ directly. In this note we will discuss one such approach i.e. SARSA or State-Action-Reward-State-Action.

**keywords** Temporal-difference-learning, SARSA, Reinforcement-learning

## SARSA

SARSA is that it's an on-policy method i.e. the agent takes actions and learns, that is it chooses the next $Q(s,\alpha)$,  under the policy $\pi$ it currently follows. 
When we update the value of $Q(s,\alpha)$ we atke into account the value $Q(s_{t+1},\alpha_{t+1})$. 
Hence, with the SARSA algorithm we work with the following tuple $(s_t, \alpha_t, s_{t+1}, \alpha_{t+1})$.

----
**Remark**

Typically, the policy $\pi$ will an $\epsilon$-greedy policy but this need not be the case.

----

The following is the update formula that SARSA is using

$$
Q(s_t, \alpha_t) = Q(s_t, \alpha_t) + \eta \left[r_{t+1} + \gamma Q(s_{t+1}, \alpha_{t+1}) - Q(s_t, \alpha_t)]
$$


## Summary


## References