# qubit-note: SARSA or State-Action-Reward-State-Action

## Overiew

<a href="2026-01-24-Temporal-Difference-Learning.md">qubit-note: Temporal Difference Learning</a> discussed temporal difference learning.
This is a compination of dynamic programming and Monte Carlo. TDL does not use the environment transition probabilities and therefore is
a model-free algorithm. One problem with TDL is that we don't which action to take; TDL approximate that state-value function $V(s)$
however, we cannot use the Bellman equations to establish the state-action value function $Q(s, \alpha)$ as we don't know the transition probabilities.
The solution thus is to approaximate $Q(s, \alpha)$ directly. In this note we will discuss one such approach i.e. SARSA or State-Action-Reward-State-Action.

**keywords** Temporal-difference-learning, SARSA, Reinforcement-learning

## SARSA

SARSA is  an on-policy method i.e. the agent takes actions and learns, that is it chooses the next $Q(s,\alpha)$,  under the policy $\pi$ it currently follows. 
When we update the value of $Q(s,\alpha)$ we take into account the value $Q(s_{t+1},\alpha_{t+1})$. 
Hence, with the SARSA algorithm we work with the following tuple $(s_t, \alpha_t, s_{t+1}, \alpha_{t+1})$.

----
**Remark**

Typically, the policy $\pi$ will an $\epsilon$-greedy policy but this need not be the case.

----

The following is the update formula that SARSA is using [1],

$$
Q(s_t, \alpha_t) = Q(s_t, \alpha_t) + \eta \left[r_{t+1} + \gamma Q(s_{t+1}, \alpha_{t+1}) - Q(s_t, \alpha_t)]
$$

The state-action value function that is learnt by SARSA reflect a real policy that includes both exploration noise and operational constraints.
The algorithm, compared to Q-learning, is more conservative and therefore convergence is slower. 

Let's have a closer look into the algorithm.

#### Step 1

Reinforcement learning algorithms will usually start by knowing nothing about the environment. So the first step is
to initialize the table table that represents $Q(s,\alpha)$ to arbitrary values; often this is just zero. This however, can also
be values that encourage exploration.

#### Step 2

The algorithm begins by some be presented with a state. SARSA needs to decide what to do whilst at this state. This is done using a policy
$\pi$ which most often will be an $\epsilon-$greedy policy.

#### Step 3

The algorithm will execute the action that was selected from step 2. The environment will respond with a reward $r_t$ and the new state $s_{t+1}$.
However, we cannot update yet the table $Q(s,\alpha)$; we need to determine what we will do next. It uses the policy $\pi$ at the new state $s_{t+1}$
in order to select the new action $\alpha_{t+1}$. $\alpha_{t+1}$ is the action that the agent will take next. 
**This is the key SARSA step that distinguishes it from Q-learning.** [1]. We can now update the state-action value function.
The update rule, see above, is using the future $Q(s_{t+1}, \alpha_{t+1})$ in order to update the current $Q(s_{t}, \alpha_{t})$

#### Step 4

After updating, we move to $s_{t+1}$ and take action $\alpha_{t+1}$ and repeat step 3. 
If we've reached a terminal state (like the end of a game or a completed transaction), the episode ends and we start fresh.
The environment has to inform the agent about whether it reached the end of the game or not. So when we take an action in the environment,
we will usually receive not just a reward signal and the next state but also a flag indicating if the end of the game or simulation has been reached. 

## Summary



This note introduces SARSA a Temporal Difference Learning on-policy algorithm for approximating the state-action value function. 
SARSA learns action values based on the same policy the agent is currently following (often $\epsilon$-greedy). Unlike Q-learning, SARSA updates $Q(s_t, \alpha_t)$ using the value of the actual next action taken $Q(s_{t+1}, \alpha_{t+1})$, making it more conservative and typically slower to converge.

The SARSA update rule is:
$$
Q(s_t, \alpha_t) \leftarrow Q(s_t, \alpha_t) + \eta \left[r_{t+1} + \gamma Q(s_{t+1}, \alpha_{t+1}) - Q(s_t, \alpha_t)\right]
$$

Algorithmically, SARSA:

1. Initializes the $Q(s_t, \alpha_t)$ table (often to zeros or exploration-friendly values).
2. Selects an action using a policy $\pi$ at the current state.
3. Executes the action, observes the reward and next state, then selects the *next action* using the same policy.
4. Updates $Q(s_t, \alpha_t)$ using the SARSA rule and repeats until a terminal state is reached.

Overall, SARSA learns a value function that reflects the true behavior policy, including exploration noise and constraints, distinguishing it clearly from off-policy methods like Q-learning.


## References

1. Hadi Aghazadeh, _Reinforcement Learning for Business_, Manning Publications